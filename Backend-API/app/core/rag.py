"""
@Author: Borja Otero Ferreira
RAG - Retrieval Augmented Generation implementation
Migraci√≥n completa desde Rag.py legacy manteniendo el flujo original
"""
import os
import glob
import re
import logging
import time
import pickle
import hashlib
import shutil
from typing import List, Dict, Tuple, Set, Any

from PyPDF2 import PdfReader
from llama_cpp import Llama
from langchain_community.vectorstores.chroma import Chroma
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.document_loaders import (
    CSVLoader, EverNoteLoader, TextLoader, UnstructuredWordDocumentLoader,
    UnstructuredEPubLoader, UnstructuredHTMLLoader, UnstructuredMarkdownLoader,
    UnstructuredODTLoader, UnstructuredPowerPointLoader,
)

from app.utils.logger import logger




class DocumentHasher:
    @staticmethod
    def compute_hash(content: str) -> str:
        """Compute a hash of the document content for deduplication."""
        return hashlib.md5(content.encode()).hexdigest()


class DocumentStore:
    def __init__(self, index_path: str):
        self.index_path = index_path
        
        # Verificar y migrar el √≠ndice de documento_index.pkl en ra√≠z si existe
        legacy_index_path = "document_index.pkl"
        if os.path.exists(legacy_index_path) and not os.path.exists(self.index_path):
            print(f"‚ö†Ô∏è Encontrado √≠ndice legacy en ra√≠z, migrando a {self.index_path}")
            logger.info(f"Encontrado √≠ndice legacy en ra√≠z, migrando a {self.index_path}")
            # Crear directorio si no existe
            parent_dir = os.path.dirname(self.index_path)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
            # Copiar el archivo
            try:
                shutil.copy2(legacy_index_path, self.index_path)
                print("‚úÖ Migraci√≥n del √≠ndice completada con √©xito")
                logger.info("Migraci√≥n del √≠ndice completada con √©xito")
            except Exception as e:
                print(f"‚ùå Error al migrar el √≠ndice: {e}")
                logger.error(f"Error al migrar el √≠ndice: {e}")
        
        self.document_index = self.load_index()
        self.document_hashes: Set[str] = {
            DocumentHasher.compute_hash(page["content"])
            for pages in self.document_index.values()
            for page in pages.values()
        }
        print(f"üîç Document store inicializado con {len(self.document_hashes)} fragmentos de documentos")
        logger.info(f"Document store inicializado con {len(self.document_hashes)} fragmentos de documentos")
        self.document_summaries = {}

    def load_index(self) -> Dict:
        if os.path.exists(self.index_path):
            try:
                print(f"üìÇ Cargando √≠ndice de documentos desde: {self.index_path}")
                logger.info(f"Cargando √≠ndice de documentos desde: {self.index_path}")
                with open(self.index_path, 'rb') as f:
                    index_data = pickle.load(f)
                print(f"‚úÖ √çndice cargado con √©xito: {len(index_data)} documentos")
                logger.info(f"√çndice cargado con √©xito: {len(index_data)} documentos")
                return index_data
            except (pickle.PickleError, EOFError) as e:
                print(f"‚ùå Error al cargar el √≠ndice de documentos: {e}")
                print(f"‚ùå Creando un nuevo √≠ndice vac√≠o")
                logger.error(f"Error al cargar el √≠ndice de documentos: {e}")
                return {}
        else:
            parent_dir = os.path.dirname(self.index_path)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
                print(f"üìÅ Directorio creado para el √≠ndice de documentos: {parent_dir}")
                logger.info(f"Directorio creado para el √≠ndice de documentos: {parent_dir}")
            print(f"‚ö†Ô∏è No se encontr√≥ el archivo de √≠ndice en: {self.index_path}")
            print(f"‚ö†Ô∏è Creando un nuevo √≠ndice vac√≠o")
            logger.warning(f"No se encontr√≥ el archivo de √≠ndice en: {self.index_path}")
            return {}

    def save_index(self):
        """Guardar √≠ndice de documentos con manejo mejorado de errores"""
        try:
            # Ensure parent directory exists
            parent_dir = os.path.dirname(self.index_path)
            if parent_dir and not os.path.exists(parent_dir):
                os.makedirs(parent_dir, exist_ok=True)
                logger.info(f"Directorio creado para guardar √≠ndice: {parent_dir}")
            
            # Guardar primero a un archivo temporal
            temp_path = f"{self.index_path}.tmp"
            with open(temp_path, 'wb') as f:
                pickle.dump(self.document_index, f)
            
            # Renombrar para una escritura at√≥mica
            if os.path.exists(self.index_path):
                os.replace(temp_path, self.index_path)
            else:
                os.rename(temp_path, self.index_path)
            
            num_docs = len(self.document_index)
            print(f"üíæ √çndice guardado con √©xito en {self.index_path} ({num_docs} documentos)")
            logger.info(f"√çndice guardado con √©xito en {self.index_path} ({num_docs} documentos)")
            
            # Verificar tama√±o del archivo para debugging
            file_size = os.path.getsize(self.index_path)
            logger.info(f"Tama√±o del archivo de √≠ndice: {file_size} bytes")
            
            return True
        except Exception as e:
            print(f"‚ùå Error al guardar el √≠ndice: {e}")
            logger.error(f"‚ùå Error al guardar el √≠ndice: {e}")
            return False

    def is_duplicate(self, content: str) -> bool:
        """Check if document content already exists in the store."""
        return DocumentHasher.compute_hash(content) in self.document_hashes

    def add_document(self, file_name: str, page_num: int, content: str, metadata: Dict):
        """Add a document to the store if it's not a duplicate."""
        content_hash = DocumentHasher.compute_hash(content)
        if content_hash not in self.document_hashes:
            if file_name not in self.document_index:
                self.document_index[file_name] = {}
            self.document_index[file_name][page_num] = {"content": content, "metadata": metadata}
            self.document_hashes.add(content_hash)
            return True
        return False


class PaginatedPDFLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Document]:
        reader = PdfReader(self.file_path)
        documents = []
        for page_num in range(len(reader.pages)):
            page = reader.pages[page_num]
            text = page.extract_text()
            documents.append(Document(
                page_content=text,
                metadata={
                    "page_num": page_num + 1,
                    "file_name": os.path.basename(self.file_path),
                    "file_path": self.file_path
                }
            ))
        return documents


class Retriever:
    LOADER_MAPPING: Dict[str, Tuple[type, Dict]] = {
        ".csv": (CSVLoader, {}),
        ".doc": (UnstructuredWordDocumentLoader, {}),
        ".docx": (UnstructuredWordDocumentLoader, {}),
        ".enex": (EverNoteLoader, {}),
        ".epub": (UnstructuredEPubLoader, {}),
        ".html": (UnstructuredHTMLLoader, {}),
        ".md": (UnstructuredMarkdownLoader, {}),
        ".odt": (UnstructuredODTLoader, {}),
        ".pdf": (PaginatedPDFLoader, {}),
        ".ppt": (UnstructuredPowerPointLoader, {}),
        ".pptx": (UnstructuredPowerPointLoader, {}),
        ".txt": (TextLoader, {"encoding": "utf8"}),
    }

    def __init__(self, model: Llama, prompt: List[Dict], socket):
        """
        Initialize the Retriever for RAG.
        
        Args:
            model: The LLM model to use
            prompt: List of chat messages
            socket: Socket connection for sending responses
        """
        self.model = model
        self.prompt = prompt
        self.socket = socket
        self.source_dir = 'documents'
        self.llm_context = 8192
        self.max_tokens = int(0.5 * self.llm_context)
        self.vectorstore_path = "chroma_db"
        
        # Importar aqu√≠ para evitar dependencia circular
        from app.core.socket_handler import SocketResponseHandler
        # Usar la clase directamente para m√©todos est√°ticos
        self.socket_handler = SocketResponseHandler
        
        logger.info("üîç RAG Retriever initialized - Usando RAG exclusivamente para la respuesta")
        print("üîç RAG Retriever initialized - Usando RAG exclusivamente para la respuesta")
        print(f"üîç DEBUG: Socket recibido: {type(self.socket)}")
        logger.info(f"üîç DEBUG: Socket recibido: {type(self.socket)}")
        
        # Test inicial del socket
        try:
            logger.info("üîç Probando conectividad del socket...")
            print("üîç Probando conectividad del socket...")
            self.socket_handler.emit_console_output(self.socket, "RAG inicializado correctamente", "info")
        except Exception as socket_test_error:
            logger.error(f"‚ùå Error en test inicial del socket: {socket_test_error}")
            print(f"‚ùå Error en test inicial del socket: {socket_test_error}")
        
        # Initialize document store in chroma_db directory
        # Ensure the directory exists
        os.makedirs(self.vectorstore_path, exist_ok=True)
        document_index_path = os.path.join(self.vectorstore_path, "document_index.pkl")
        self.doc_store = DocumentStore(document_index_path)
        logger.info(f"üîç Document store inicializado desde {self.doc_store.index_path}")
        print(f"üîç Document store inicializado desde {self.doc_store.index_path}")
        
        try:
            # Load and process documents
            logger.info("üîç Cargando documentos...")
            print("üîç Cargando documentos...")
            self.docs = self.load_documents(self.source_dir)
            logger.info(f"üîç {len(self.docs)} documentos cargados")
            print(f"üîç {len(self.docs)} documentos cargados")
            
            logger.info("üîç Configurando vectorstore...")
            print("üîç Configurando vectorstore...")
            self.setup_vectorstore()
            
            if self.is_new_documents():
                logger.info("üîç Indexando documentos nuevos...")
                print("üîç Indexando documentos nuevos...")
                self.index_documents()
                self.doc_store.save_index()
                logger.info("üîç Indexaci√≥n completada")
                print("üîç Indexaci√≥n completada")

            logger.info("üîç Preparando historial de chat para RAG...")
            print("üîç Preparando historial de chat para RAG...")
            self.prepare_chat_history()
            logger.info("üîç Generando respuesta RAG...")
            print("üîç Generando respuesta RAG...")
            self.emitir_respuesta()
        except Exception as e:
            logger.error(f"Error in RAG processing: {e}")
            # Usar SocketResponseHandler para enviar error
            self.socket_handler.emit_rag_error(
                self.socket, 
                f"Error procesando documentos en RAG: {str(e)}"
            )

    def load_documents(self, source_dir: str) -> List[Document]:
        """Load documents with deduplication."""
        logger.info(f"Loading documents from {source_dir}")
        
        if not os.path.exists(source_dir):
            logger.warning(f"Directory {source_dir} does not exist. Creating it.")
            os.makedirs(source_dir)
            return []
        
        # Check if directory is empty
        files_in_dir = os.listdir(source_dir)
        logger.info(f"üîç DEBUG: Files in {source_dir}: {files_in_dir}")
            
        all_files = (
            f for ext in self.LOADER_MAPPING 
            for f in glob.glob(os.path.join(source_dir, f"**/*{ext}"), recursive=True)
        )
        
        all_files_list = list(all_files)
        logger.info(f"üîç DEBUG: Found {len(all_files_list)} document files: {all_files_list}")
        
        documents = []
        for file_path in all_files_list:
            try:
                logger.info(f"üîç DEBUG: Loading file: {file_path}")
                loaded_docs = self.load_single_document(file_path)
                logger.info(f"üîç DEBUG: Loaded {len(loaded_docs)} documents from {file_path}")
                # Only add non-duplicate documents
                for doc in loaded_docs:
                    if not self.doc_store.is_duplicate(doc.page_content):
                        documents.append(doc)
                        self.doc_store.add_document(
                            doc.metadata["file_name"],
                            doc.metadata["page_num"],
                            doc.page_content,
                            doc.metadata
                        )
                # Free memory after processing each file
                del loaded_docs
            except Exception as e:
                logger.error(f"Error loading document {file_path}: {e}")
                
        logger.info(f"Loaded {len(documents)} documents")
        return documents

    def load_single_document(self, file_path: str) -> List[Document]:
        """Load a single document using the appropriate loader."""
        ext = os.path.splitext(file_path)[-1].lower()
        if ext in self.LOADER_MAPPING:
            loader_class, loader_args = self.LOADER_MAPPING[ext]
            loader = loader_class(file_path, **loader_args)
            return loader.load()
        else:
            raise ValueError(f"Unsupported file extension '{ext}'")

    def setup_vectorstore(self):
        """Initialize the vector store with embeddings."""
        logger.info("Initializing vector store")
        
        if not self.docs:
            logger.warning("No documents to index. Creating empty vector store.")
            # Initialize empty vectorstore
            embeddings = GPT4AllEmbeddings(
                model_name="all-MiniLM-L6-v2.gguf2.f16.gguf",
                gpt4all_kwargs={'allow_download': 'True'},
                device="cuda"
            )
            self.vectorstore = Chroma(
                collection_name="rag-chroma",
                embedding_function=embeddings,
                persist_directory=self.vectorstore_path,
            )
            self.retriever = self.vectorstore.as_retriever()
            return
            
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1024
        )
        all_splits = text_splitter.split_documents(self.docs)
        
        # Free memory after splitting documents
        del self.docs
        
        # Initialize embeddings
        model_name = "all-MiniLM-L6-v2.gguf2.f16.gguf"
        embeddings = GPT4AllEmbeddings(
            model_name=model_name,
            gpt4all_kwargs={'allow_download': 'True'},
            device="cuda"
        )
        
        # Create vectorstore with error handling
        try:
            self.vectorstore = Chroma.from_documents(
                documents=all_splits,
                collection_name="rag-chroma",
                embedding=embeddings,
                persist_directory=self.vectorstore_path,
            )
            self.retriever = self.vectorstore.as_retriever()
            logger.info("Vector store initialized successfully")
        except RuntimeError as e:
            if "Cannot open header file" in str(e):
                logger.warning(f"ChromaDB corrupted, recreating database at {self.vectorstore_path}")
                # Remove corrupted database
                if os.path.exists(self.vectorstore_path):
                    shutil.rmtree(self.vectorstore_path)
                # Recreate vectorstore
                self.vectorstore = Chroma.from_documents(
                    documents=all_splits,
                    collection_name="rag-chroma",
                    embedding=embeddings,
                    persist_directory=self.vectorstore_path,
                )
                self.retriever = self.vectorstore.as_retriever()
                logger.info("Vector store recreated successfully")
            else:
                raise

    def is_new_documents(self) -> bool:
        """Check if new documents need to be indexed."""
        return True  # Simplified for now, always reindex

    def index_documents(self):
        """Index documents (placeholder)."""
        logger.info("Indexing documents")
        # Nothing to do here, indexing is done in setup_vectorstore

    def prepare_chat_history(self):
        """Prepare chat history for RAG (identical to legacy)."""
        logger.info("üîç Preparando historial de chat para RAG")
        print("üîç Preparando historial de chat para RAG")
        
        question = self.prompt[-1]['content']
        
        logger.info(f"üîç Pregunta RAG: {question}")
        print(f"üîç Pregunta RAG: {question}")
        
        # Mejorar la detecci√≥n de referencias a p√°ginas y documentos (like legacy)
        page_match = re.search(r'p√°gina (\d+)', question, re.IGNORECASE)
        file_match = re.search(r'(?:en|del|de|documento|pdf) ([^\.]+\.pdf)', question, re.IGNORECASE)
        
        if page_match and file_match:
            page_num = int(page_match.group(1))
            file_name = file_match.group(1).strip()
            
            logger.info(f"üîç Referencia expl√≠cita detectada: documento '{file_name}', p√°gina {page_num}")
            print(f"üîç Referencia expl√≠cita detectada: documento '{file_name}', p√°gina {page_num}")
            
            # Buscar espec√≠ficamente en la p√°gina solicitada
            content = self.get_page_content(file_name, page_num)
            if content != "P√°gina no encontrada":
                # Si hay una consulta espec√≠fica, buscar en la p√°gina
                query_terms = re.sub(r'p√°gina \d+|(?:en|del|de|documento|pdf) [^\.]+\.pdf', '', question).strip()
                if query_terms:
                    logger.info(f"üîç Buscando t√©rminos espec√≠ficos: '{query_terms}' en {file_name}, p√°gina {page_num}")
                    print(f"üîç Buscando t√©rminos espec√≠ficos: '{query_terms}' en {file_name}, p√°gina {page_num}")
                    content = self.search_in_page(file_name, page_num, query_terms)
                
                from langchain.docstore.document import Document
                doc = Document(
                    page_content=content,
                    metadata={"file_name": file_name, "page_num": page_num}
                )
                docs = [doc]
                
                logger.info(f"üîç Usando contenido espec√≠fico de p√°gina: {len(content)} caracteres")
                print(f"üîç Usando contenido espec√≠fico de p√°gina: {len(content)} caracteres")
            else:
                logger.warning(f"üîç No se encontr√≥ la p√°gina {page_num} en el documento {file_name}, realizando b√∫squeda de similitud")
                print(f"üîç No se encontr√≥ la p√°gina {page_num} en el documento {file_name}, realizando b√∫squeda de similitud")
                docs = self.vectorstore.similarity_search(question.lower(), k=10)
        elif file_match:
            file_name = file_match.group(1).strip()
            
            logger.info(f"üîç Referencia a documento detectada: '{file_name}'")
            print(f"üîç Referencia a documento detectada: '{file_name}'")
            
            query_terms = re.sub(r'(?:en|del|de|documento|pdf) [^\.]+\.pdf', '', question).strip()
            if query_terms:
                logger.info(f"üîç Buscando t√©rminos espec√≠ficos: '{query_terms}' en todo el documento {file_name}")
                print(f"üîç Buscando t√©rminos espec√≠ficos: '{query_terms}' en todo el documento {file_name}")
                
                search_results = self.search_in_document(file_name, query_terms)
                from langchain.docstore.document import Document
                docs = [
                    Document(page_content=content, metadata={"file_name": file_name, "page_num": page_num})
                    for page_num, content in search_results.items()
                ]
                
                logger.info(f"üîç Encontradas {len(search_results)} p√°ginas con coincidencias")
                print(f"üîç Encontradas {len(search_results)} p√°ginas con coincidencias")
            else:
                logger.info(f"üîç No hay t√©rminos espec√≠ficos, realizando b√∫squeda de similitud en {file_name}")
                print(f"üîç No hay t√©rminos espec√≠ficos, realizando b√∫squeda de similitud en {file_name}")
                docs = self.vectorstore.similarity_search(question.lower(), k=10)
        else:
            logger.info("üîç No se detectaron referencias a documentos espec√≠ficos, realizando b√∫squeda de similitud general")
            print("üîç No se detectaron referencias a documentos espec√≠ficos, realizando b√∫squeda de similitud general")
            docs = self.vectorstore.similarity_search(question.lower(), k=10)
        
        logger.info(f"üîç Se encontraron {len(docs)} documentos relevantes")
        print(f"üîç Se encontraron {len(docs)} documentos relevantes")
        
        # Check if the documents fit into the context
        if self.fits_in_context(docs):
            logger.info("üîç Los documentos caben en el contexto sin truncar")
            print("üîç Los documentos caben en el contexto sin truncar")
            doc_txt = self.format_docs(docs)
        else:
            # Perform similarity search if documents don't fit in context
            logger.info("üîç Los documentos exceden el tama√±o del contexto, truncando")
            print("üîç Los documentos exceden el tama√±o del contexto, truncando")
            docs = self.truncate_docs(docs, self.max_tokens)
            doc_txt = self.format_docs(docs)
            logger.info(f"üîç Documentos truncados a {len(docs)}")
            print(f"üîç Documentos truncados a {len(docs)}")
        
        logger.info(f"üîç Longitud del texto de documentos: {len(doc_txt)} caracteres")
        print(f"üîç Longitud del texto de documentos: {len(doc_txt)} caracteres")
        
        # Mostrar una vista previa del contexto que se enviar√° al modelo
        preview = doc_txt[:200] + "..." if len(doc_txt) > 200 else doc_txt
        logger.info(f"üîç Vista previa del contexto: {preview}")
        print(f"üîç Vista previa del contexto: {preview}")
        
        # Construir el mensaje del sistema para el modelo
        system_message = f"""Eres un asistente de IA especializado en b√∫squeda de informaci√≥n en documentos. 
        
        IMPORTANTE: Debes SOLO utilizar la informaci√≥n que se encuentra en los siguientes fragmentos de documentos para responder a la pregunta. 
        NO utilices conocimientos generales ni informaci√≥n externa.
        
        Si la informaci√≥n NO est√° en los fragmentos proporcionados, responde claramente: 
        "Lo siento, no encuentro informaci√≥n sobre esto en los documentos disponibles."
        
        No menciones que est√°s utilizando fragmentos ni hables sobre el formato de tu respuesta.
        No pidas informaci√≥n adicional.
        No inventes informaci√≥n.
        
        Documentos: \n\n {doc_txt} \n\n
        
        Pregunta: {question}\n"""

        self.chat_history = [{"role": "system", "content": system_message}]
        
        # A√±adir un mensaje del usuario con la pregunta para asegurar que el modelo responde a ella
        prompt_with_instruction = f"Por favor, responde a mi pregunta bas√°ndote √öNICAMENTE en los documentos proporcionados: {question}"
        self.chat_history.append({"role": "user", "content": prompt_with_instruction})
        
        logger.info(f"üîç Historial de chat creado con mensaje del sistema de {len(system_message)} caracteres")
        print(f"üîç Historial de chat creado con mensaje del sistema de {len(system_message)} caracteres")
        
        logger.info(f"üîç DEBUG: Chat history created with system message length: {len(system_message)}")
        
        # Free memory after preparing chat history
        del docs

    def get_page_content(self, file_name: str, page_num: int) -> str:
        """Obtiene el contenido de una p√°gina espec√≠fica de un documento """
        if file_name in self.doc_store.document_index:
            if page_num in self.doc_store.document_index[file_name]:
                return self.doc_store.document_index[file_name][page_num]["content"]
        return "P√°gina no encontrada"

    def search_in_page(self, file_name: str, page_num: int, query: str) -> str:
        """Busca un t√©rmino espec√≠fico en una p√°gina particular """
        content = self.get_page_content(file_name, page_num)
        if content != "P√°gina no encontrada":
            # Realizar b√∫squeda de similitud en el contenido de la p√°gina
            return self.perform_similarity_search(content, query)
        return "P√°gina no encontrada"

    def perform_similarity_search(self, content: str, query: str) -> str:
        """Realiza una b√∫squeda de similitud en un contenido espec√≠fico"""
        # Crear un documento temporal para la b√∫squeda
        from langchain.docstore.document import Document
        temp_doc = Document(page_content=content)
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            chunk_size=1024
        )
        splits = text_splitter.split_documents([temp_doc])
        
        # Crear una colecci√≥n temporal en el vectorstore
        temp_vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=self.vectorstore._embedding_function,
            collection_name="temp_search"
        )
        
        # Realizar b√∫squeda
        results = temp_vectorstore.similarity_search(query, k=1)
        
        # Limpiar la colecci√≥n temporal
        temp_vectorstore = None
        
        return results[0].page_content if results else "No se encontraron resultados relevantes"

    def search_in_document(self, file_name: str, query: str) -> Dict[int, str]:
        """Busca un t√©rmino espec√≠fico en todas las p√°ginas de un documento """
        if file_name in self.doc_store.document_index:
            results = {}
            for page_num, page_data in self.doc_store.document_index[file_name].items():
                content = page_data["content"]
                if query.lower() in content.lower():
                    results[page_num] = content
            return results
        return {"error": "Documento no encontrado"}

    def fits_in_context(self, docs: List[Document]) -> bool:
        """Check if the documents fit into the context size"""
        total_tokens = sum(len(doc.page_content.split()) for doc in docs)
        return total_tokens <= self.max_tokens

    def truncate_docs(self, docs: List[Document], max_tokens: int) -> List[Document]:
        """Truncate documents to fit in context """
        truncated_docs = []
        total_tokens = 0
        for doc in docs:
            doc_tokens = len(doc.page_content.split())
            if total_tokens + doc_tokens > max_tokens:
                remaining_tokens = max_tokens - total_tokens
                truncated_content = ' '.join(
                    doc.page_content.split()[:remaining_tokens]
                )
                truncated_docs.append(
                    Document(
                        page_content=truncated_content, 
                        metadata=doc.metadata
                    )
                )
                break
            truncated_docs.append(doc)
            total_tokens += doc_tokens
        return truncated_docs

    def format_docs(self, docs: List[Document]) -> str:
        """Format documents for display """
        return "\n\n".join(
            f"{doc.metadata['file_name']} - p√°gina {doc.metadata.get('page_num', 'N/A')}\n{doc.page_content.lower()}" 
            for doc in docs
        )

    def emitir_respuesta(self):
        """Generate and emit RAG response."""
        response_completa = ''
        total_tokens = 0
        
        try:
            logger.info("üîç Generando respuesta RAG exclusivamente (sin modelo base)")
            print("üîç Generando respuesta RAG exclusivamente (sin modelo base)")
            
            # Use chat_history instead of rag_messages (like legacy)
            if not hasattr(self, 'chat_history') or not self.chat_history:
                logger.error("No chat history to process")
                print("‚ùå Error: No hay historial de chat para procesar en RAG")
                # Socket directo como en legacy
                self.socket.emit('assistant_response', {
                    'content': "Error: No se pudo procesar la consulta RAG.",
                    'finished': True,
                    'error': True
                }, namespace='/test')
                return
            
            logger.info(f"üîç Enviando prompt al modelo con {len(self.chat_history[0]['content'])} caracteres")
            print(f"üîç Enviando prompt al modelo con {len(self.chat_history[0]['content'])} caracteres")
            
            # Stream exactly like legacy - chunk by chunk manually con socket directo
            for chunk in self.model.create_chat_completion(
                messages=self.chat_history,
                max_tokens=10000,
                stream=True
            ):
                if 'content' in chunk['choices'][0]['delta']:
                    fragmento_response = chunk['choices'][0]['delta']['content']
                    response_completa += fragmento_response
                    total_tokens += 1
                    
                    # Emit EXACTO como legacy - socket directo
                    self.socket.emit(
                        'assistant_response',
                        {'content': fragmento_response},
                        namespace='/test'
                    )
                    
                    import time
                    time.sleep(0.01)
            
            logger.info(f"üîç Respuesta RAG completada con √©xito - {total_tokens} tokens generados")
            print(f"üîç Respuesta RAG completada con √©xito - {total_tokens} tokens generados")
            
            # Send finalization signal EXACTO como legacy - socket directo
            # Calcular tokens del usuario con manejo de excepciones
            try:
                user_tokens = len(' '.join([msg['content'] for msg in self.prompt if 'content' in msg]).split())
            except Exception:
                user_tokens = 0
                logger.warning("‚ö†Ô∏è Error al calcular tokens del usuario, usando 0")
            
            self.socket.emit('assistant_response', {
                'content': '',
                'total_user_tokens': user_tokens,
                'total_assistant_tokens': total_tokens,
                'finished': True
            }, namespace='/test')
            
            logger.info("üîç Se√±al de finalizaci√≥n enviada")
            print("üîç Se√±al de finalizaci√≥n enviada")
            
        except Exception as e:
            logger.error(f"‚ùå Error en RAG response: {e}")
            print(f"‚ùå Error en RAG response: {e}")
            # Enviar error EXACTO como legacy - socket directo
            self.socket.emit('assistant_response', {
                'content': f"Error en RAG: {str(e)}",
                'finished': True,
                'error': True
            }, namespace='/test')

    def search_in_document(self, file_name: str, query: str) -> Dict[int, str]:
        """Busca en todo un documento """
        if file_name in self.doc_store.document_index:
            results = {}
            for page_num, page_data in self.doc_store.document_index[file_name].items():
                content = page_data["content"]
                if query.lower() in content.lower():
                    results[page_num] = content
            return results
        return {}

    def fits_in_context(self, docs: List[Document]) -> bool:
        """Check if documents fit in context """
        total_length = sum(len(doc.page_content) for doc in docs)
        return total_length <= self.max_tokens * 4  # Rough estimate

    def format_docs(self, docs: List[Document]) -> str:
        """Format documents for context with improved structure."""
        formatted_docs = []
        
        for i, doc in enumerate(docs, 1):
            filename = doc.metadata.get('file_name', 'Documento sin nombre')
            page_num = doc.metadata.get('page_num', 'N/A')
            
            # Formato claro con separadores y numeraci√≥n
            doc_header = f"--- DOCUMENTO {i}: {filename} (p√°gina {page_num}) ---"
            formatted_docs.append(f"{doc_header}\n{doc.page_content}")
        
        return "\n\n".join(formatted_docs)

    def truncate_docs(self, docs: List[Document], max_tokens: int) -> List[Document]:
        """Truncate documents to fit in context"""
        truncated = []
        total_length = 0
        for doc in docs:
            if total_length + len(doc.page_content) <= max_tokens * 4:
                truncated.append(doc)
                total_length += len(doc.page_content)
            else:
                break
        return truncated
