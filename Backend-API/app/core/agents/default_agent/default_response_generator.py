"""
@Author: Borja Otero Ferreira
Default Response Generator - Generador de respuestas finales para el agente default
Migrado desde la l√≥gica de generaci√≥n de respuestas de Cortex
"""
from typing import Any, List, Tuple
from colorama import Fore, Style
import queue
import threading

from app.utils.logger import logger
from .config import DefaultAgentConfig


class DefaultResponseGenerator:
    """
    Generador de respuestas finales para el agente default
    Migrado desde Cortex.generar_respuesta_final
    """
    
    def __init__(self, model, socket, response_queue, original_prompt, assistant, config: DefaultAgentConfig):
        self.model = model
        self.socket = socket
        self.response_queue = response_queue
        self.original_prompt = original_prompt
        self.assistant = assistant
        self.config = config
        self.engine_lock = threading.Lock()
    
    def generar_respuesta_final(self, resultados_herramientas: List[Tuple[str, str, str]], emit_status_callback=None) -> str:
        """
        Genera la respuesta final incorporando los resultados de todas las herramientas
        Migrado desde Cortex.generar_respuesta_final
        """
        salida = '[>] Generando respuesta final incorporando todos los resultados...‚úçÔ∏è'
        print(f"{Fore.GREEN}{salida}{Style.RESET_ALL}")
        logger.info(salida)
        
        if emit_status_callback:
            emit_status_callback(salida)
        else:
            self._enviar_a_consola(salida, 'info')
        
        try:
            # Crear el prompt para generar la respuesta final
            final_prompt = self._crear_prompt_respuesta_final(resultados_herramientas)
            
            # Generar la respuesta final usando create_chat_completion como en Cortex
            response_content = ""
            for chunk in self.model.create_chat_completion(messages=final_prompt, max_tokens=1024, stream=True):
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        response_content += delta['content']
            
            final_response = response_content.strip()
            
            # Verificar que la respuesta no est√© vac√≠a
            if not final_response or final_response.strip() == "":
                final_response = self._generar_respuesta_fallback(resultados_herramientas)
            
            logger.info("‚úÖ Respuesta final generada exitosamente")
            return final_response
            
        except Exception as e:
            logger.error(f"Error generando respuesta final: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return self._generar_respuesta_fallback(resultados_herramientas)
    
    def _crear_prompt_respuesta_final(self, resultados_herramientas: List[Tuple[str, str, str]]) -> List[dict]:
        """
        Crear el prompt para generar la respuesta final
        """
        # Construir informaci√≥n de resultados
        resultados_info = ""
        if resultados_herramientas:
            resultados_info = "\n## INFORMACI√ìN RECOPILADA:\n\n"
            for i, (funcion, query, resultado) in enumerate(resultados_herramientas, 1):
                resultados_info += f"**Fuente {i} ({funcion}):**\n"
                resultados_info += f"Consulta: {query}\n"
                resultados_info += f"Resultado: {resultado}\n\n"
        
        # Instrucciones para la respuesta final
        instrucciones = """
## INSTRUCCIONES PARA LA RESPUESTA:
1. Integra TODA la informaci√≥n recopilada de manera coherente y √∫til
2. Organiza la informaci√≥n de forma clara y estructurada
3. Responde directamente a la consulta original del usuario
4. Si hay m√∫ltiples fuentes, compara y sintetiza la informaci√≥n
5. Usa formato Markdown para mejorar la legibilidad
6. NO menciones las herramientas utilizadas internamente
7. Presenta la informaci√≥n como si fuera tu conocimiento directo
8. Si la informaci√≥n es contradictoria, menciona las diferentes perspectivas

## CONSULTA ORIGINAL:
"""
        
        # Construir el prompt completo
        prompt_content = instrucciones + f"{self.original_prompt}\n" + resultados_info
        
        messages = [
            {"role": "system", "content": "Eres un asistente experto que integra informaci√≥n de m√∫ltiples fuentes para dar respuestas completas y √∫tiles. Tu objetivo es crear una respuesta cohesiva y bien estructurada."},
            {"role": "user", "content": prompt_content}
        ]
        
        return messages
    
    def _generar_respuesta_fallback(self, resultados_herramientas: List[Tuple[str, str, str]]) -> str:
        """
        Generar una respuesta de fallback si hay error en la generaci√≥n principal
        """
        if not resultados_herramientas:
            return "No se pudo obtener informaci√≥n adicional para responder a tu consulta."
        
        respuesta_fallback = f"Bas√°ndome en la informaci√≥n recopilada sobre tu consulta '{self.original_prompt}':\n\n"
        
        for i, (funcion, query, resultado) in enumerate(resultados_herramientas, 1):
            respuesta_fallback += f"**Informaci√≥n {i}:**\n"
            respuesta_fallback += f"{resultado}\n\n"
        
        respuesta_fallback += "Espero que esta informaci√≥n sea √∫til para responder a tu consulta."
        
        return respuesta_fallback
    
    def _generate_normal_response(self, model: Any) -> str:
        """
        Generar respuesta normal sin herramientas
        Migrado desde Cortex._generate_normal_response
        """
        try:
            logger.info("üìù Generando respuesta normal sin herramientas")
            
            # Crear prompt simple para respuesta directa
            messages = [
                {"role": "system", "content": "Eres un asistente √∫til. Responde de manera clara y precisa bas√°ndote en tu conocimiento."},
                {"role": "user", "content": self.original_prompt}
            ]
            
            # Generar respuesta usando create_chat_completion
            response_content = ""
            for chunk in model.create_chat_completion(messages=messages, max_tokens=1024, stream=True):
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        response_content += delta['content']
            
            logger.info("‚úÖ Respuesta normal generada exitosamente")
            return response_content.strip()
            
        except Exception as e:
            logger.error(f"Error generando respuesta normal: {e}")
            return f"Lo siento, no pude generar una respuesta adecuada. Error: {e}"
    
    def _enviar_a_consola(self, mensaje: str, rol: str):
        """Enviar mensaje a consola usando SocketResponseHandler"""
        if self.config.ENABLE_SOCKET_EMISSION:
            try:
                from app.core.socket_handler import SocketResponseHandler
                SocketResponseHandler.emit_console_output(self.socket, mensaje, rol)
            except Exception as e:
                logger.error(f"Error emitting to socket: {e}")
    
    def hablar_response(self, response: str):
        """
        Funci√≥n para hablar la respuesta usando TTS
        Migrado desde Cortex.hablar_response
        """
        if not self.config.VOICE_RESPONSE_ENABLED:
            return
            
        try:
            # Importar pyttsx3 solo cuando sea necesario
            import pyttsx3
            
            # Limitar la longitud del texto a hablar
            max_length = 500  # caracteres
            if len(response) > max_length:
                response = response[:max_length] + "..."
            
            with self.engine_lock:
                try:
                    engine = pyttsx3.init()
                    
                    # Configurar velocidad y volumen
                    engine.setProperty('rate', 150)  # velocidad de habla
                    engine.setProperty('volume', 0.8)  # volumen (0.0 a 1.0)
                    
                    # Configurar voz en espa√±ol si est√° disponible
                    voices = engine.getProperty('voices')
                    for voice in voices:
                        if 'spanish' in voice.name.lower() or 'es' in voice.id.lower():
                            engine.setProperty('voice', voice.id)
                            break
                    
                    # Hablar el texto
                    engine.say(response)
                    engine.runAndWait()
                    engine.stop()
                    
                except Exception as e:
                    logger.error(f"Error en TTS engine: {e}")
                    
        except ImportError:
            logger.warning("pyttsx3 no est√° disponible para TTS")
        except Exception as e:
            logger.error(f"Error en hablar_response: {e}")
